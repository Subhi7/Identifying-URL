{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install cdx_toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdx_toolkit\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import io\n",
    "import gzip\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_domain(domain):\n",
    "    record_list = []\n",
    "    index_list = ['29','24']\n",
    "    index_list = [\"2020-\"+i for i in index_list]\n",
    "    print (\"Trying target domain: {}\".format(domain))\n",
    "    for index in index_list:\n",
    "        print (\"Trying index {}\".format(index))\n",
    "        cc_url  = \"http://index.commoncrawl.org/CC-MAIN-%s-index?\" % index\n",
    "        cc_url += \"url=%s&matchType=domain&output=json\" % domain\n",
    "        response = requests.get(cc_url)\n",
    "        if response.status_code == 200:\n",
    "            records = response.content.splitlines()\n",
    "            for record in records:\n",
    "                record_list.append(json.loads(record))  \n",
    "            print(\"Added {} results.\".format(len(records)))\n",
    "    print(\"Found a total of {} hits.\".format(len(record_list)))\n",
    "    return record_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying target domain: https://www.henryford.com/news\n",
      "Trying index 2020-29\n",
      "Added 12109 results.\n",
      "Found a total of 12109 hits.\n"
     ]
    }
   ],
   "source": [
    "record = search_domain(\"https://www.henryford.com/news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'urlkey': 'com,henryford)/',\n",
       " 'timestamp': '20200709183308',\n",
       " 'status': '200',\n",
       " 'url': 'https://www.henryford.com/',\n",
       " 'mime': 'text/html',\n",
       " 'digest': 'L67VRN4WNCAVYQFG4RYQDCGPZZK3SWOQ',\n",
       " 'charset': 'UTF-8',\n",
       " 'offset': '918916517',\n",
       " 'filename': 'crawl-data/CC-MAIN-2020-29/segments/1593655900614.47/warc/CC-MAIN-20200709162634-20200709192634-00208.warc.gz',\n",
       " 'length': '18857',\n",
       " 'mime-detected': 'text/html',\n",
       " 'languages': 'eng'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_page(record):\n",
    "\n",
    "    offset, length = int(record['offset']), int(record['length'])\n",
    "    offset_end = offset + length - 1\n",
    "\n",
    "    # We'll get the file via HTTPS so we don't need to worry about S3 credentials\n",
    "    # Getting the file on S3 is equivalent however - you can request a Range\n",
    "    prefix = 'https://commoncrawl.s3.amazonaws.com/'\n",
    "\n",
    "    # We can then use the Range header to ask for just this set of bytes\n",
    "    resp = requests.get(prefix + record['filename'], headers={'Range': 'bytes={}-{}'.format(offset, offset_end)})\n",
    "\n",
    "    # The page is stored compressed (gzip) to save space\n",
    "    # We can extract it using the GZIP library\n",
    "    raw_data = io.BytesIO(resp.content)\n",
    "    f = gzip.GzipFile(fileobj=raw_data)\n",
    "\n",
    "    # What we have now is just the WARC response, formatted:\n",
    "    data = f.read()\n",
    "\n",
    "    response = \"\"\n",
    "\n",
    "    if len(data):\n",
    "        try:\n",
    "            warc, header, response = data.strip().decode(\"utf-8\").split('\\r\\n\\r\\n', 2)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in record:\n",
    "    res.append(download_page(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['detroit treatment with hydroxychloroquine cut the death rate significantly in sick patients hospitalized with covid 19 and without']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in [re.sub(r'\\W+', ' ', i).strip().lower() for i in [t.text for t in BeautifulSoup(res[0], \"html.parser\").find_all('p')]] if any(x in t for x in ['hydroxychloroquine','chloroquine']) & ('covid' in 'hydroxychloroquine covid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.find('body') for i in BeautifulSoup(res[0], \"html.parser\").find_all('body', {'class':'henryford'}, limit=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just couldn't get the URL name for the individual page. Does the beautiful soup output not have that?\n",
    "\n",
    "Instead I am returning, for the domains we have, how many total number of webpages that have misinformation present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible webpage with misinformation found!\n",
      "Confirming if this was the case\n",
      "Confirmed that this was indeed malicious.\n",
      "Possible webpage with misinformation found!\n",
      "Confirming if this was the case\n",
      "Confirmed that this was indeed malicious.\n",
      "Possible webpage with misinformation found!\n",
      "Confirming if this was the case\n",
      "Confirmed that this was indeed malicious.\n",
      "For the domains we have, the total number of webpages that have the misinformation present: 3\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for r in res:\n",
    "    if len([t for t in [re.sub(r'\\W+', ' ', i).strip().lower() for i in [t.text for t in BeautifulSoup(r, \"html.parser\").find_all('p')]] if any(x in t for x in ['hydroxychloroquine','chloroquine']) & ('covid' in 'hydroxychloroquine covid')]):\n",
    "        print(\"Possible webpage with misinformation found!\")\n",
    "        text_list = [re.sub(r'\\W+', ' ', i).strip().lower() for i in [t.text for t in BeautifulSoup(r, \"html.parser\").find_all('p')]]\n",
    "        text_combined = ' '.join(text_list)\n",
    "        \n",
    "        print(\"Confirming if this was the case\")\n",
    "        if (TextBlob(text_combined).sentiment.polarity)>0:\n",
    "            print(\"Confirmed that this was indeed malicious.\")\n",
    "            count+=1\n",
    "print(\"For the domains we have, the total number of webpages that have the misinformation present:\",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When the range of Textblob polarity is from -1 to +1 hence this is a positive sentiment\n",
    "\n",
    "When the website is giving a positive sentiment, it is a website which is promoting hydroxychloroquine or chloroquine for treatment of COVID.\n",
    "This code can likewise be tested for other websites too in order to check its validity for misinformation\n",
    "\n",
    "Thus I provide the count of webpages having misinformation in these domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code\n",
    "\n",
    "Running for multiple domains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [\"https://www.henryford.com/news\",\"https://www.hss.edu/\",\"https://www.sciencenews.org/article\"]:\n",
    "    record = search_domain(d)\n",
    "    res = []\n",
    "    for i in record:\n",
    "        res.append(download_page(i))\n",
    "    count=0\n",
    "    for r in res:\n",
    "        if len([t for t in [re.sub(r'\\W+', ' ', i).strip().lower() for i in [t.text for t in BeautifulSoup(r, \"html.parser\").find_all('p')]] if any(x in t for x in ['hydroxychloroquine','chloroquine']) & ('covid' in 'hydroxychloroquine covid')]):\n",
    "            print(\"Possible webpage with misinformation found!\")\n",
    "            text_list = [re.sub(r'\\W+', ' ', i).strip().lower() for i in [t.text for t in BeautifulSoup(r, \"html.parser\").find_all('p')]]\n",
    "            text_combined = ' '.join(text_list)\n",
    "\n",
    "            print(\"Confirming if this was the case\")\n",
    "            if (TextBlob(text_combined).sentiment.polarity)>0:\n",
    "                print(\"Confirmed that this was indeed malicious.\")\n",
    "                count+=1\n",
    "    print(\"For the domain \",d,\" the total number of webpages that have the misinformation present:\",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
